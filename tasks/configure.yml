---

# Needed to ensure some services start properly
- name: Set hostname
  become: yes
  lineinfile:
    dest: /etc/hosts
    line: "127.0.0.1 {{ ansible_hostname }}"
  notify:
    - restart kafka

- name: Copy AWS autodiscover script
  become: yes
  become_user: "{{ kafka.user }}"
  copy:
    src: aws_cluster_autodiscover
    dest: "/home/{{ kafka.user }}/bin/aws_cluster_autodiscover"
    owner: "{{ kafka.user }}"
    group: "{{ kafka.group }}"
    mode: 0750
  when: kafka.aws_cluster_autodiscover.enabled

- name: Run AWS autodiscover script and grab cluster settings
  become: yes
  become_user: "{{ kafka.user }}"
  command: "./aws_cluster_autodiscover {{ kafka.aws_cluster_autodiscover.hosts | join(',') }} {{ kafka.aws_cluster_autodiscover.r53_zone_id }} \"{{ kafka.aws_cluster_autodiscover.lookup_filter }}\" {{ kafka.aws_cluster_autodiscover.id_tag_name}}"
  args:
    chdir: "/home/{{ kafka.user }}/bin"
  register: aws_cluster_autodiscover
  until: aws_cluster_autodiscover | success
  retries: 4
  delay: 10
  when: kafka.aws_cluster_autodiscover.enabled

# Combine is used with set fact as successive calls to the module
# seem to remove previous values added to hashes
- name: Set cluster facts based on AWS autodiscover script output
  set_fact:
    kafka: "{{ kafka | combine( {
        'aws_cluster_autodiscover': {
          'data': aws_cluster_autodiscover.stdout | from_json
        }
      }, recursive=True) }}"
  when: kafka.aws_cluster_autodiscover.enabled

- name: Export DNS hostname
  become: yes
  become_user: "{{ kafka.user }}"
  lineinfile:
    create: yes
    dest: "/home/{{ kafka.user }}/etc/hostname"
    line: "{{ kafka.aws_cluster_autodiscover.data.hostname }}"
  when: kafka.aws_cluster_autodiscover.enabled

- name: Export hosted zone ID from aws_cluster_autodiscover settings
  become: yes
  become_user: "{{ kafka.user }}"
  lineinfile:
    create: yes
    dest: "/home/{{ kafka.user }}/etc/hostedZone"
    line: "{{ kafka.aws_cluster_autodiscover.r53_zone_id }}"
  when: kafka.aws_cluster_autodiscover.enabled

- name: Export broker ID to local filesystem
  become: yes
  become_user: "{{ kafka.user }}"
  lineinfile:
    create: yes
    dest: "/home/{{ kafka.user }}/etc/brokerId"
    line: "{{ kafka.aws_cluster_autodiscover.data.id }}"
  when: kafka.aws_cluster_autodiscover.enabled

- name: Assigned ID EC2 fact hunt
  action: ec2_facts
  register: ec2_vars
  until: ec2_vars | success
  retries: 3
  delay: 5
  when: kafka.aws_cluster_assigned_id.enabled

- name: Assigned ID grab tags from EC2 instance
  ec2_tag:
    region: "{{ ansible_ec2_placement_region }}"
    resource: "{{ ansible_ec2_instance_id }}"
    state: list
  register: assigned_id_instance_tags
  when: ansible_ec2_instance_id is defined
    and kafka.aws_cluster_assigned_id.enabled
  retries: 3
  delay: 5
  until: assigned_id_instance_tags | success

- name: Assigned ID set Broker ID
  set_fact:
    kafka: "{{ kafka | combine( {
        'id': assigned_id_instance_tags.tags[kafka.aws_cluster_assigned_id.id_tag_name]
      }, recursive=True) }}"
  when: ansible_ec2_instance_id is defined
    and kafka.aws_cluster_assigned_id.enabled

- name: Grab local facts for looking up interfaces
  action: setup
  register: local_facts
  when: kafka.interface_bind
    or kafka.interface_advertise

- name: Set listen_address if interface bind set
  set_fact:
    kafka: "{{ kafka | combine( {
        'listen_address': local_facts.ansible_facts['ansible_' + kafka.interface_bind]['ipv4']['address']
      }, recursive=True) }}"
  when: kafka.interface_bind

- name: Set advertised_host_name if interface advertise set
  set_fact:
    kafka: "{{ kafka | combine( {
        'advertised_host_name': local_facts.ansible_facts['ansible_' + kafka.interface_advertise]['ipv4']['address']
      }, recursive=True) }}"
  when: kafka.interface_advertise

- name: Setup environment config
  become: yes
  become_user: "{{ kafka.user }}"
  template:
    dest: "/home/{{ kafka.user }}/etc/environment"
    mode: 0644
    src: environment.j2
  notify:
    - restart kafka

- name: Create log4j.properties
  become: yes
  become_user: "{{ kafka.user }}"
  template:
    dest: "/home/{{ kafka.user }}/etc/log4j.properties"
    mode: 0644
    src: log4j.properties.j2
  notify:
    - restart kafka

- name: Create server.properties
  become: yes
  become_user: "{{ kafka.user }}"
  template:
    dest: "/home/{{ kafka.user }}/etc/server.properties"
    mode: 0640
    src: server.properties.j2
  notify:
    - restart kafka

- name: Ensure Kafka is running
  become: yes
  service:
    name: kafka
    state: started

- name: Flush handlers to ensure Kafka is up to date
  meta: flush_handlers

- name: Wait for Kafka port
  wait_for:
    port: "{{ kafka.port }}"
    state: started
    timeout: "{{ kafka.wait_for_kafka_port }}"

- name: Copy Kafka maintenance scripts
  become: yes
  become_user: "{{ kafka.user }}"
  copy:
    src: "{{ item }}"
    mode: 0750
    dest: "/home/{{ kafka.user }}/bin/{{ item }}"
  with_items:
    - kafka_maintenance_at_stop
    - kafka_maintenance_at_start
    - remove_dns_record
  when: kafka.aws_cluster_autodiscover.enabled

- name: Run Kafka maintenance script at start
  become: yes
  become_user: "{{ kafka.user }}"
  shell: "/home/{{ kafka.user }}/bin/kafka_maintenance_at_start"
  when: kafka.aws_cluster_autodiscover.enabled

- name: Copy Kafka stop script
  become: yes
  copy:
    dest: "/etc/init.d"
    mode: 0755
    src: "stop_kafka"
  when: kafka.aws_cluster_autodiscover.enabled

- name: Add Kafka stop script to system runlevels
  become: yes
  file:
    src: "/etc/init.d/stop_kafka"
    dest: "/etc/{{ item }}/K10kafka-controlled-shutdown"
    state: link
    force: yes
  with_items:
    - "rc0.d"
    - "rc1.d"
    - "rc6.d"
  when: kafka.aws_cluster_autodiscover.enabled
